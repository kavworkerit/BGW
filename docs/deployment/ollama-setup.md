# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Ollama –¥–ª—è LLM –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏

## üìã –û–±–∑–æ—Ä

BGW –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å Ollama –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. LLM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è:
- –ò–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞
- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–∞–∑–≤–∞–Ω–∏–π –∏–≥—Ä
- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å–æ–±—ã—Ç–∏–π
- –ì–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω–æ–Ω–∏–º–æ–≤

## üöÄ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama

### –ù–∞ —Ö–æ—Å—Ç-–º–∞—à–∏–Ω–µ (–¥–ª—è Docker)

1. **–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Ollama:**
   ```bash
   # Linux/macOS
   curl -fsSL https://ollama.ai/install.sh | sh

   # –ò–ª–∏ —Å–∫–∞—á–∞–π—Ç–µ —Å https://ollama.ai/download
   ```

2. **–ó–∞–ø—É—Å—Ç–∏—Ç–µ Ollama —Å–µ—Ä–≤–∏—Å:**
   ```bash
   ollama serve
   ```

3. **–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–∞–±–æ—Ç—É:**
   ```bash
   curl http://localhost:11434/api/tags
   ```

### –í Docker (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ)

–ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å Ollama –≤ Docker:

```yaml
# –î–æ–±–∞–≤—å—Ç–µ –≤ docker-compose.yml
ollama:
  image: ollama/ollama
  ports:
    - "11434:11434"
  volumes:
    - ollama_data:/root/.ollama
  restart: unless-stopped

volumes:
  ollama_data:
```

## üéÆ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–µ–π

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏

1. **Llama 3.2 (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è):**
   ```bash
   ollama pull llama3.2:3b
   ```

2. **Mistral (—Ö–æ—Ä–æ—à–æ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ):**
   ```bash
   ollama pull mistral
   ```

3. **Qwen (–æ—Ç–ª–∏—á–Ω–æ –¥–ª—è –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ/—Ä—É—Å—Å–∫–æ–≥–æ):**
   ```bash
   ollama pull qwen2.5:3b
   ```

4. **CodeLlama (–¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á):**
   ```bash
   ollama pull codellama
   ```

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π

```bash
# –û—Å–Ω–æ–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏
ollama pull llama3.2:3b
ollama pull mistral
ollama pull qwen2.5:3b

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
ollama list
```

## ‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ BGW

### 1. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è URL

–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤ `backend/app/core/config.py` —É–∫–∞–∑–∞–Ω –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π URL:

```python
OLLAMA_URL: Optional[str] = "http://host.docker.internal:11434"
```

### 2. –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è

–î–æ–±–∞–≤—å—Ç–µ –≤ `.env` —Ñ–∞–π–ª:

```bash
# Ollama settings
OLLAMA_URL=http://host.docker.internal:11434
OLLAMA_MODEL=llama3.2:3b
```

### 3. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–µ—Ä–≤–∏—Å—ã

```bash
docker-compose restart api worker
```

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏

### 1. –ß–µ—Ä–µ–∑ API

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç—É—Å
curl http://localhost:8000/api/llm/status

# –ü–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏
curl http://localhost:8000/api/llm/status

# –¢–µ—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
curl -X POST http://localhost:8000/api/llm/game/extract \
  -H "Content-Type: application/json" \
  -d '{"text": "–ì—Ä–æ–º–∫–æ–µ –¥–µ–ª–æ - –Ω–æ–≤–∞—è –∏–≥—Ä–∞ –∑–∞ 2500 ‚ÇΩ"}'
```

### 2. –ß–µ—Ä–µ–∑ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å

1. –û—Ç–∫—Ä–æ–π—Ç–µ BGW –≤ –±—Ä–∞—É–∑–µ—Ä–µ
2. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ "–ù–∞—Å—Ç—Ä–æ–π–∫–∏" ‚Üí "LLM (Ollama)"
3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç–∞—Ç—É—Å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
4. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏

### 3. –ß–µ—Ä–µ–∑ —Ç–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç

```bash
cd /path/to/BGW
python test_llm.py
```

## üîß –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –∞–≥–µ–Ω—Ç–∞—Ö

LLM –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –∞–≥–µ–Ω—Ç–∞—Ö –ø—Ä–∏ –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞:

### –ü—Ä–∏–º–µ—Ä –≤ –∞–≥–µ–Ω—Ç–µ:

```python
# –í –º–µ—Ç–æ–¥–µ parse –∞–≥–µ–Ω—Ç–∞
if self.confidence < 0.7:
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º LLM –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    llm_result = await llm_service.extract_game_info(text)
    if llm_result:
        return self.convert_to_listing_event(llm_result)
```

## üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### –ú–µ—Ç—Ä–∏–∫–∏

- –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ LLM
- –ß–∞—Å—Ç–æ—Ç–∞ –≤—ã–∑–æ–≤–æ–≤
- –£—Å–ø–µ—à–Ω–æ—Å—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤

### –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

LLM —Å–µ—Ä–≤–∏—Å –ª–æ–≥–∏—Ä—É–µ—Ç:
- –£—Å–ø–µ—à–Ω—ã–µ/–Ω–µ—É—Å–ø–µ—à–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
- –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
- –û—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–æ–≤

## üö® –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ–∏—Å–ø—Ä–∞–≤–Ω–æ—Å—Ç–µ–π

### Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω

1. **–ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω:**
   ```bash
   ps aux | grep ollama
   ```

2. **–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ—Ä—Ç:**
   ```bash
   netstat -tlnp | grep 11434
   ```

3. **–ü—Ä–æ–≤–µ—Ä—å—Ç–µ Docker —Å–µ—Ç–µ–≤–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ:**
   ```bash
   docker run --rm curlimages/curl curl http://host.docker.internal:11434/api/tags
   ```

### –ú–µ–¥–ª–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã

1. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–æ–ª–µ–µ –º–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏:**
   ```bash
   ollama pull llama3.2:1b
   ```

2. **–ù–∞—Å—Ç—Ä–æ–π—Ç–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –≤ LLM —Å–µ—Ä–≤–∏—Å–µ:**
   ```python
   # –í app/services/llm_service.py
   "temperature": 0.1,  # —Å–Ω–∏–∑—å—Ç–µ –¥–ª—è –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
   ```

### –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã

1. **–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–æ–º–ø—Ç—ã** –≤ `app/services/llm_service.py`
2. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–æ–ª–µ–µ –º–æ—â–Ω—ã–µ –º–æ–¥–µ–ª–∏** –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á
3. **–î–æ–±–∞–≤—å—Ç–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –ø—Ä–æ–º–ø—Ç—ã** –¥–ª—è –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

## üîí –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å

1. **–õ–æ–∫–∞–ª—å–Ω–æ–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ** - Ollama —Ä–∞–±–æ—Ç–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ
2. **–ù–µ—Ç –ø–µ—Ä–µ–¥–∞—á–∏ –¥–∞–Ω–Ω—ã—Ö** –Ω–∞ –≤–Ω–µ—à–Ω–∏–µ —Å–µ—Ä–≤–∏—Å—ã
3. **–ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏** - –≤—ã –≤—ã–±–∏—Ä–∞–µ—Ç–µ –∫–∞–∫–∏–µ –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
4. **–ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–µ—Ç—å** - Docker –∏–∑–æ–ª–∏—Ä—É–µ—Ç Ollama –æ—Ç –≤–Ω–µ—à–Ω–µ–≥–æ –¥–æ—Å—Ç—É–ø–∞

## üìà –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

### –î–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:

1. **–í—ã–±–µ—Ä–∏—Ç–µ –ø–æ–¥—Ö–æ–¥—è—â—É—é –º–æ–¥–µ–ª—å:**
   - `llama3.2:1b` - –±—ã—Å—Ç—Ä–∞—è, –±–∞–∑–æ–≤—ã–µ –∑–∞–¥–∞—á–∏
   - `llama3.2:3b` - –±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏/–∫–∞—á–µ—Å—Ç–≤–∞
   - `mistral` - —Ö–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ

2. **–ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:**
   ```python
   "temperature": 0.1,    # –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
   "max_tokens": 500,     # –æ–≥—Ä–∞–Ω–∏—á—å—Ç–µ –¥–ª–∏–Ω—É
   "top_p": 0.9          # –Ω–µ–º–Ω–æ–≥–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
   ```

3. **–ö—ç—à–∏—Ä—É–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã** –¥–ª—è –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –∑–∞–ø—Ä–æ—Å–æ–≤

## üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

- [Ollama Documentation](https://github.com/ollama/ollama)
- [Available Models](https://ollama.ai/library)
- [Llama 3.2 Models](https://ollama.ai/library/llama3.2)

## üÜò –ü–æ–¥–¥–µ—Ä–∂–∫–∞

–ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã:

1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏: `docker-compose logs api`
2. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω –∏ –¥–æ—Å—Ç—É–ø–µ–Ω
3. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ —á–µ—Ä–µ–∑ `test_llm.py`
4. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å–µ—Ç–∏ Docker